{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample: I don\\'t have the anonymized data yet, but this is how it should work for entropy and granularity (I think, I havn\\'t tested it yet)\\n\\nquasi_identifiers = [\\'QI1\\', \\'QI2\\', \\'QI3\\']  # Adjust based on your dataset\\n\\n# I\\'m not sure if we need the calculations for the original dataset (maybe for comparison)\\noriginal_entropy = calculate_entropy(original_df, quasi_identifiers)\\noriginal_granularity = calculate_granularity(original_df, quasi_identifiers)\\n\\n# For anonymized dataset\\nanonymized_entropy = calculate_entropy(anonymized_df, quasi_identifiers)\\nanonymized_granularity = calculate_granularity(anonymized_df, quasi_identifiers)\\n\\n# Print results\\nprint(f\"Original Entropy: {original_entropy:.4f}\")\\nprint(f\"Anonymized Entropy: {anonymized_entropy:.4f}\")\\nprint(f\"Increase in Entropy: {anonymized_entropy - original_entropy:.4f}\\n\")\\n\\nprint(f\"Original Granularity: {original_granularity:.4f}\")\\nprint(f\"Anonymized Granularity: {anonymized_granularity:.4f}\")\\nprint(f\"Decrease in Granularity: {original_granularity - anonymized_granularity:.4f}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_entropy(df, quasi_identifiers):\n",
    "    eq_class_counts = df.groupby(quasi_identifiers).size()\n",
    "    total_records = len(df)\n",
    "    probabilities = eq_class_counts / total_records\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def calculate_granularity(df, quasi_identifiers):\n",
    "    eq_class_counts = df.groupby(quasi_identifiers).size()\n",
    "    total_records = len(df)\n",
    "    granularity = total_records / len(eq_class_counts)\n",
    "    return granularity\n",
    "\n",
    "\"\"\"\n",
    "Number of records changed\n",
    "Another useful statistic is the number of records changed per variable. These can be counted in a similar way as the missing values and include suppressions (i.e., changes to missing/’NA’ in R). \n",
    "The number of records changed gives a good indication of the impact of the anonymization methods on the data. Listing 41 illustrates how to compute the number of records changed for the PRAMmed variables.\n",
    "https://sdcpractice.readthedocs.io/en/latest/utility.html \n",
    "\"\"\"\n",
    "def information_loss(original_df, anonymized_df, quasi_identifiers):\n",
    "    changes = (original_df[quasi_identifiers] != anonymized_df[quasi_identifiers]).sum().sum()\n",
    "    total_values = original_df[quasi_identifiers].size\n",
    "    return changes / total_values\n",
    "\n",
    "\"\"\"\n",
    "IL1s information loss measure: only for continous variables\n",
    "https://sdcpractice.readthedocs.io/en/latest/utility.html \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_IL1s(original_df, anonymized_df, continuous_columns):\n",
    "    n = len(original_df)  # Number of records\n",
    "    p = len(continuous_columns)  # Number of continuous variables\n",
    "\n",
    "    total_loss = 0\n",
    "    for col in continuous_columns:\n",
    "        std_dev = original_df[col].std()  \n",
    "        if std_dev == 0:\n",
    "            print(\"Warning: std_dev is zero\")  \n",
    "\n",
    "        # Sum of absolute differences between original and anonymized values\n",
    "        col_loss = np.sum(np.abs(original_df[col] - anonymized_df[col])) / np.sqrt(2 * std_dev)\n",
    "        total_loss += col_loss\n",
    "\n",
    "    return (1 / (p * n)) * total_loss if p * n > 0 else 0  \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Example: I don't have the anonymized data yet, but this is how it should work for entropy and granularity (I think, I havn't tested it yet)\n",
    "\n",
    "quasi_identifiers = ['QI1', 'QI2', 'QI3']  # Adjust based on your dataset\n",
    "\n",
    "# I'm not sure if we need the calculations for the original dataset (maybe for comparison)\n",
    "original_entropy = calculate_entropy(original_df, quasi_identifiers)\n",
    "original_granularity = calculate_granularity(original_df, quasi_identifiers)\n",
    "\n",
    "# For anonymized dataset\n",
    "anonymized_entropy = calculate_entropy(anonymized_df, quasi_identifiers)\n",
    "anonymized_granularity = calculate_granularity(anonymized_df, quasi_identifiers)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Entropy: {original_entropy:.4f}\")\n",
    "print(f\"Anonymized Entropy: {anonymized_entropy:.4f}\")\n",
    "print(f\"Increase in Entropy: {anonymized_entropy - original_entropy:.4f}\\n\")\n",
    "\n",
    "print(f\"Original Granularity: {original_granularity:.4f}\")\n",
    "print(f\"Anonymized Granularity: {anonymized_granularity:.4f}\")\n",
    "print(f\"Decrease in Granularity: {original_granularity - anonymized_granularity:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Transformation\n",
      "\n",
      "Entropy: 5.676970518353784\n",
      "Granularity: 329.8611111111111\n",
      "\n",
      "Local Transformation\n",
      "\n",
      "Entropy: 10.366108971551206\n",
      "Granularity: 8.996212121212121\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "### CREDIT DEFAULT ###\n",
    "######################\n",
    "\n",
    "### Global Transformation ###\n",
    "print('Global Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "CCD_train = pd.read_csv('data/CCD_train_GT.csv', sep=\",\")\n",
    "quasi_identifiers = ['LIMIT_BAL', 'EDUCATION', 'BILL_AMT1', 'PAY_AMT1']\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(CCD_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(CCD_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)\n",
    "\n",
    "### Local Transformation ###\n",
    "print('')\n",
    "print('Local Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "CCD_train = pd.read_csv('data/CCD_train_LT.csv', sep=\",\")\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(CCD_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(CCD_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Transformation\n",
      "\n",
      "Entropy: 5.262647227928238\n",
      "Granularity: 1808.2833333333333\n",
      "\n",
      "Local Transformation\n",
      "\n",
      "Entropy: 11.443320930352215\n",
      "Granularity: 3.4214758751182592\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### CENSUS INCOME ###\n",
    "#####################\n",
    "\n",
    "### Global Transformation ###\n",
    "print('Global Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "KDD_train = pd.read_csv('data/KDD_train_GT.csv', sep=\",\")\n",
    "quasi_identifiers = ['age', 'marital_stat', 'birth_country_mother', 'education']\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(KDD_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(KDD_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)\n",
    "\n",
    "### Local Transformation ###\n",
    "print('')\n",
    "print('Local Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "KDD_train = pd.read_csv('data/KDD_train_LT.csv', sep=\",\")\n",
    "\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(KDD_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(KDD_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Transformation\n",
      "\n",
      "Entropy: 3.832967871991527\n",
      "Granularity: 37.25\n",
      "\n",
      "Local Transformation\n",
      "\n",
      "Entropy: 6.056525146541469\n",
      "Granularity: 8.895522388059701\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "### PIMA INDIAN ###\n",
    "###################\n",
    "\n",
    "### Global Transformation ###\n",
    "print('Global Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "PID_train = pd.read_csv('data/PID_train_GT.csv', sep=\",\")\n",
    "quasi_identifiers = ['Age', 'BloodPressure', 'Insulin']\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(PID_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(PID_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)\n",
    "\n",
    "### Local Transformation ###\n",
    "print('')\n",
    "print('Local Transformation')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "PID_train = pd.read_csv('data/PID_train_LT.csv', sep=\",\")\n",
    "\n",
    "# Entropy\n",
    "entropy = calculate_entropy(PID_train, quasi_identifiers)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "# Granularity\n",
    "granularity = calculate_granularity(PID_train, quasi_identifiers)\n",
    "print(\"Granularity:\", granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
